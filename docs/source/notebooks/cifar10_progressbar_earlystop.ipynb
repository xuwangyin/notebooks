{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch Training With Early Stopping and Progress Bar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cifar_model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The content of cifar_model.py\n",
    "\n",
    "'''ResNet in PyTorch.\n",
    "For Pre-activation ResNet, see 'preact_resnet.py'.\n",
    "Reference:\n",
    "[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n",
    "    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n",
    "'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils import spectral_norm\n",
    "\n",
    "\n",
    "class FakeReLU(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        return input.clamp(min=0)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        return grad_output\n",
    "\n",
    "\n",
    "class SequentialWithArgs(torch.nn.Sequential):\n",
    "    def forward(self, input, *args, **kwargs):\n",
    "        vs = list(self._modules.values())\n",
    "        l = len(vs)\n",
    "        for i in range(l):\n",
    "            if i == l - 1:\n",
    "                input = vs[i](input, *args, **kwargs)\n",
    "            else:\n",
    "                input = vs[i](input)\n",
    "        return input\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride,\n",
    "                               padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1,\n",
    "                               padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion * planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1,\n",
    "                          stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion * planes))\n",
    "\n",
    "    def forward(self, x, fake_relu=False):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        if fake_relu:\n",
    "            return FakeReLU.apply(out)\n",
    "        return F.relu(out)\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
    "                               padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion * planes, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion * planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion * planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion * planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x, fake_relu=False):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        if fake_relu:\n",
    "            return FakeReLU.apply(out)\n",
    "        return F.relu(out)\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    # feat_scale lets us deal with CelebA, other non-32x32 datasets\n",
    "    def __init__(self, block, num_blocks, num_classes=10, feat_scale=1, wm=1, dataset='cifar10'):\n",
    "        super(ResNet, self).__init__()\n",
    "\n",
    "        assert dataset in ['cifar10', 'celeba128']\n",
    "        first_stride = {'cifar10': 1, 'celeba128': 2}[dataset]\n",
    "\n",
    "        widths = [64, 128, 256, 512]\n",
    "        widths = [int(w * wm) for w in widths]\n",
    "\n",
    "        self.in_planes = widths[0]\n",
    "        self.conv1 = nn.Conv2d(3, self.in_planes, kernel_size=3, stride=first_stride,\n",
    "                               padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(self.in_planes)\n",
    "        self.layer1 = self._make_layer(block, widths[0], num_blocks[0], stride=first_stride)\n",
    "        self.layer2 = self._make_layer(block, widths[1], num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, widths[2], num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, widths[3], num_blocks[3], stride=2)\n",
    "        self.layer5 = self._make_layer(block, widths[3], num_blocks[3], stride=2)\n",
    "        # self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.pool = nn.AvgPool2d(4)\n",
    "        self.linear = nn.Linear(feat_scale * widths[3] * block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return SequentialWithArgs(*layers)\n",
    "\n",
    "    def forward(self, x, with_latent=False, fake_relu=False, no_relu=False):\n",
    "        assert (not no_relu), \\\n",
    "            \"no_relu not yet supported for this architecture\"\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out, fake_relu=fake_relu)\n",
    "        out = self.pool(out)\n",
    "        pre_out = out.view(out.size(0), -1)\n",
    "        final = self.linear(pre_out)\n",
    "        if with_latent:\n",
    "            return final, pre_out\n",
    "        return final\n",
    "\n",
    "\n",
    "def ResNet18(**kwargs):\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n",
    "\n",
    "\n",
    "def ResNet18Wide(**kwargs):\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2], wd=1.5, **kwargs)\n",
    "\n",
    "\n",
    "def ResNet18Thin(**kwargs):\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2], wd=.75, **kwargs)\n",
    "\n",
    "\n",
    "def ResNet34(**kwargs):\n",
    "    return ResNet(BasicBlock, [3, 4, 6, 3], **kwargs)\n",
    "\n",
    "\n",
    "def ResNet50(**kwargs):\n",
    "    return ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n",
    "\n",
    "\n",
    "def ResNet101(**kwargs):\n",
    "    return ResNet(Bottleneck, [3, 4, 23, 3], **kwargs)\n",
    "\n",
    "\n",
    "def ResNet152(**kwargs):\n",
    "    return ResNet(Bottleneck, [3, 8, 36, 3], **kwargs)\n",
    "\n",
    "\n",
    "resnet50 = ResNet50\n",
    "resnet18 = ResNet18\n",
    "resnet101 = ResNet101\n",
    "resnet152 = ResNet152"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from cifar_model import resnet50\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('1.4.0', '0.5.0')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__, torchvision.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val(model, optimizer, train_loader, val_loader, epochs, earlystop_patience=-1):\n",
    "    model.train()\n",
    "    val_accs = []\n",
    "    best_val_loss = np.inf\n",
    "    patience_counter = 0\n",
    "    for epoch in range(epochs):\n",
    "        # Train\n",
    "        train_loss, train_preds, train_labels = [], [], []\n",
    "        model.train()\n",
    "        pbar = tqdm(train_loader, position=0, leave=True)\n",
    "        for i, (data, label) in enumerate(pbar):                                            \n",
    "            data, label = data.to(device), label.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(data)\n",
    "            loss = F.cross_entropy(logits, label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            acc = (preds == label).type(torch.FloatTensor).mean()\n",
    "            train_loss.append(loss.item()), train_preds.append(preds), train_labels.append(label)\n",
    "            train_loss_mean = torch.Tensor(train_loss).mean()\n",
    "            train_acc_mean = (torch.cat(train_preds) == \n",
    "                              torch.cat(train_labels)).type(torch.FloatTensor).mean().item()\n",
    "            pbar.set_postfix({'epoch': epoch, \n",
    "                              'loss': f'{train_loss_mean:.2f}', \n",
    "                              'acc': f'{train_acc_mean:.2f}'})\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss, val_preds, val_labels = [], [], []\n",
    "        pbar = tqdm(val_loader, position=0, leave=True)\n",
    "        for i, (data, label) in enumerate(pbar):\n",
    "            data, label = data.to(device), label.to(device)\n",
    "            with torch.no_grad():\n",
    "                logits = model(data)\n",
    "                loss = F.cross_entropy(logits, label).item()\n",
    "                preds = torch.argmax(logits, dim=1)\n",
    "                acc = (preds == label).type(torch.FloatTensor).mean().item()\n",
    "                val_loss.append(loss), val_preds.append(preds), val_labels.append(label)\n",
    "            val_loss_mean = torch.Tensor(val_loss).mean()\n",
    "            val_acc_mean = (torch.cat(val_preds) == \n",
    "                            torch.cat(val_labels)).type(torch.FloatTensor).mean().item()\n",
    "            pbar.set_postfix({'epoch': epoch, \n",
    "                              'val loss': f'{val_loss_mean:.2f}', \n",
    "                              'val acc': f'{val_acc_mean:.2f}'})\n",
    "            \n",
    "        if val_loss_mean < best_val_loss:\n",
    "            best_val_loss = val_loss_mean\n",
    "            patience_counter = 0\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if earlystop_patience >= 0 and patience_counter > earlystop_patience:\n",
    "                break\n",
    "    if earlystop_patience >= 0:\n",
    "        model.load_state_dict(best_model_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=True, download=True, transform=transform_train)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=128, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=False, download=True, transform=transform_test)\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=100, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet50(num_classes=len(trainset.classes))\n",
    "model = torch.nn.DataParallel(model)\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [01:09<00:00,  5.60it/s, epoch=0, loss=1.53, acc=0.44]\n",
      "100%|██████████| 100/100 [00:07<00:00, 13.40it/s, epoch=0, val loss=1.16, val acc=0.60]\n",
      "100%|██████████| 391/391 [01:00<00:00,  6.43it/s, epoch=1, loss=1.03, acc=0.64]\n",
      "100%|██████████| 100/100 [00:07<00:00, 13.16it/s, epoch=1, val loss=0.91, val acc=0.68]\n",
      "100%|██████████| 391/391 [01:01<00:00,  6.33it/s, epoch=2, loss=0.81, acc=0.72]\n",
      "100%|██████████| 100/100 [00:07<00:00, 13.12it/s, epoch=2, val loss=0.81, val acc=0.72]\n",
      "100%|██████████| 391/391 [01:00<00:00,  6.42it/s, epoch=3, loss=0.70, acc=0.76]\n",
      "100%|██████████| 100/100 [00:07<00:00, 13.21it/s, epoch=3, val loss=0.80, val acc=0.74]\n",
      "100%|██████████| 391/391 [01:00<00:00,  6.43it/s, epoch=4, loss=0.62, acc=0.79]\n",
      "100%|██████████| 100/100 [00:07<00:00, 13.09it/s, epoch=4, val loss=0.90, val acc=0.70]\n",
      "100%|██████████| 391/391 [01:00<00:00,  6.44it/s, epoch=5, loss=0.57, acc=0.80]\n",
      "100%|██████████| 100/100 [00:07<00:00, 12.90it/s, epoch=5, val loss=0.64, val acc=0.78]\n",
      "100%|██████████| 391/391 [01:01<00:00,  6.35it/s, epoch=6, loss=0.54, acc=0.82]\n",
      "100%|██████████| 100/100 [00:07<00:00, 12.97it/s, epoch=6, val loss=0.55, val acc=0.81]\n",
      "100%|██████████| 391/391 [01:01<00:00,  6.31it/s, epoch=7, loss=0.52, acc=0.83]\n",
      "100%|██████████| 100/100 [00:07<00:00, 12.87it/s, epoch=7, val loss=0.85, val acc=0.75]\n",
      "100%|██████████| 391/391 [01:02<00:00,  6.31it/s, epoch=8, loss=0.49, acc=0.83]\n",
      "100%|██████████| 100/100 [00:08<00:00, 12.50it/s, epoch=8, val loss=0.62, val acc=0.79]\n",
      "100%|██████████| 391/391 [01:03<00:00,  6.20it/s, epoch=9, loss=0.48, acc=0.84]\n",
      "100%|██████████| 100/100 [00:07<00:00, 13.07it/s, epoch=9, val loss=0.64, val acc=0.79]\n",
      "100%|██████████| 391/391 [01:01<00:00,  6.41it/s, epoch=10, loss=0.45, acc=0.85]\n",
      "100%|██████████| 100/100 [00:07<00:00, 13.14it/s, epoch=10, val loss=0.65, val acc=0.78]\n",
      "100%|██████████| 391/391 [01:01<00:00,  6.38it/s, epoch=11, loss=0.43, acc=0.85]\n",
      "100%|██████████| 100/100 [00:07<00:00, 13.51it/s, epoch=11, val loss=0.49, val acc=0.84]\n",
      "100%|██████████| 391/391 [01:00<00:00,  6.45it/s, epoch=12, loss=0.42, acc=0.86]\n",
      "100%|██████████| 100/100 [00:07<00:00, 13.61it/s, epoch=12, val loss=0.48, val acc=0.84]\n",
      "100%|██████████| 391/391 [01:00<00:00,  6.46it/s, epoch=13, loss=0.41, acc=0.86]\n",
      "100%|██████████| 100/100 [00:07<00:00, 12.56it/s, epoch=13, val loss=0.49, val acc=0.83]\n",
      "100%|██████████| 391/391 [01:00<00:00,  6.42it/s, epoch=14, loss=0.39, acc=0.87]\n",
      "100%|██████████| 100/100 [00:07<00:00, 13.28it/s, epoch=14, val loss=0.45, val acc=0.86]\n",
      "100%|██████████| 391/391 [01:00<00:00,  6.44it/s, epoch=15, loss=0.37, acc=0.87]\n",
      "100%|██████████| 100/100 [00:07<00:00, 14.01it/s, epoch=15, val loss=0.44, val acc=0.86]\n",
      "100%|██████████| 391/391 [01:01<00:00,  6.36it/s, epoch=16, loss=0.36, acc=0.88]\n",
      "100%|██████████| 100/100 [00:07<00:00, 12.87it/s, epoch=16, val loss=0.44, val acc=0.86]\n",
      "100%|██████████| 391/391 [01:00<00:00,  6.42it/s, epoch=17, loss=0.35, acc=0.88]\n",
      "100%|██████████| 100/100 [00:07<00:00, 13.25it/s, epoch=17, val loss=0.41, val acc=0.86]\n",
      "100%|██████████| 391/391 [01:00<00:00,  6.46it/s, epoch=18, loss=0.34, acc=0.88]\n",
      "100%|██████████| 100/100 [00:07<00:00, 13.40it/s, epoch=18, val loss=0.42, val acc=0.86]\n",
      "100%|██████████| 391/391 [01:00<00:00,  6.43it/s, epoch=19, loss=0.33, acc=0.89]\n",
      "100%|██████████| 100/100 [00:07<00:00, 13.58it/s, epoch=19, val loss=0.49, val acc=0.84]\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4, weight_decay=5e-4)\n",
    "train_val(model, optimizer, train_loader, val_loader, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [01:00<00:00,  6.42it/s, epoch=0, loss=0.21, acc=0.93]\n",
      "100%|██████████| 100/100 [00:07<00:00, 12.76it/s, epoch=0, val loss=0.25, val acc=0.92]\n",
      "100%|██████████| 391/391 [01:00<00:00,  6.44it/s, epoch=1, loss=0.17, acc=0.94]\n",
      "100%|██████████| 100/100 [00:07<00:00, 13.12it/s, epoch=1, val loss=0.25, val acc=0.92]\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=5e-4)\n",
    "train_val(model, optimizer, train_loader, val_loader, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [01:00<00:00,  6.42it/s, epoch=0, loss=0.15, acc=0.95]\n",
      "100%|██████████| 100/100 [00:07<00:00, 13.56it/s, epoch=0, val loss=0.24, val acc=0.92]\n",
      "100%|██████████| 391/391 [01:01<00:00,  6.41it/s, epoch=1, loss=0.14, acc=0.95]\n",
      "100%|██████████| 100/100 [00:07<00:00, 13.62it/s, epoch=1, val loss=0.24, val acc=0.92]\n",
      "100%|██████████| 391/391 [01:00<00:00,  6.44it/s, epoch=2, loss=0.13, acc=0.96]\n",
      "100%|██████████| 100/100 [00:07<00:00, 13.27it/s, epoch=2, val loss=0.24, val acc=0.93]\n",
      "100%|██████████| 391/391 [01:00<00:00,  6.46it/s, epoch=3, loss=0.12, acc=0.96]\n",
      "100%|██████████| 100/100 [00:07<00:00, 13.43it/s, epoch=3, val loss=0.25, val acc=0.92]\n",
      "100%|██████████| 391/391 [01:01<00:00,  6.40it/s, epoch=4, loss=0.12, acc=0.96]\n",
      "100%|██████████| 100/100 [00:07<00:00, 13.44it/s, epoch=4, val loss=0.25, val acc=0.92]\n",
      "100%|██████████| 391/391 [01:00<00:00,  6.45it/s, epoch=5, loss=0.11, acc=0.96]\n",
      "100%|██████████| 100/100 [00:07<00:00, 12.52it/s, epoch=5, val loss=0.24, val acc=0.92]\n",
      "100%|██████████| 391/391 [01:00<00:00,  6.43it/s, epoch=6, loss=0.11, acc=0.97]\n",
      "100%|██████████| 100/100 [00:07<00:00, 12.90it/s, epoch=6, val loss=0.26, val acc=0.92]\n",
      "100%|██████████| 391/391 [01:00<00:00,  6.42it/s, epoch=7, loss=0.10, acc=0.97]\n",
      "100%|██████████| 100/100 [00:07<00:00, 12.74it/s, epoch=7, val loss=0.24, val acc=0.93]\n",
      "100%|██████████| 391/391 [01:01<00:00,  6.36it/s, epoch=8, loss=0.10, acc=0.97]\n",
      "100%|██████████| 100/100 [00:07<00:00, 13.13it/s, epoch=8, val loss=0.25, val acc=0.92]\n",
      "100%|██████████| 391/391 [01:03<00:00,  6.16it/s, epoch=9, loss=0.10, acc=0.97]\n",
      "100%|██████████| 100/100 [00:07<00:00, 13.08it/s, epoch=9, val loss=0.25, val acc=0.93]\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=5e-4)\n",
    "train_val(model, optimizer, train_loader, val_loader, epochs=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
